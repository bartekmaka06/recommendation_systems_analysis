{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d9a3964",
   "metadata": {},
   "source": [
    "Przypadki testowe:\n",
    "\n",
    "dla małej i dużej liczby filmów\n",
    "\n",
    "dla małej i dużej liczby użytkowników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3689121b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, KNNBasic\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from time import time\n",
    "import psutil\n",
    "from surprise import SVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "\n",
    "ml_rating_path = 'dane/ml-20m/ml-20m/ratings.csv'\n",
    "ml_movies_path = 'dane/ml-20m/ml-20m/movies.csv'\n",
    "ml_tags_path = 'dane/ml-20m/ml-20m/tags.csv'\n",
    "\n",
    "# Funkcja do ograniczenia liczby filmów\n",
    "def reduce_movies(data, movie_count):\n",
    "    sampled_movies = data['movieId'].unique()[:movie_count]\n",
    "    reduced_data = data[data['movieId'].isin(sampled_movies)]\n",
    "    return reduced_data\n",
    "\n",
    "# Funkcja do obliczania metryk\n",
    "def calculate_metrics(predictions, threshold=4.0):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for pred in predictions:\n",
    "        true_rating = pred.r_ui\n",
    "        predicted_rating = pred.est\n",
    "        y_true.append(1 if true_rating >= threshold else 0)\n",
    "        y_pred.append(1 if predicted_rating >= threshold else 0)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b88953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Size: 50, Movie Count: 100\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User Size: 50, Movie Count: 1000\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User Size: 50, Movie Count: 5000\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User Size: 1000, Movie Count: 100\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User Size: 1000, Movie Count: 1000\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "User Size: 1000, Movie Count: 5000\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "   user_size  movie_count  precision    recall  f1_score  time (s)  \\\n",
      "0         50          100   0.725275  0.628571  0.673469  0.062355   \n",
      "1         50         1000   0.643750  0.448802  0.528883  0.011994   \n",
      "2         50         5000   0.649165  0.379358  0.478873  0.015961   \n",
      "3       1000          100   0.740245  0.575639  0.647647  0.691177   \n",
      "4       1000         1000   0.737207  0.406535  0.524070  2.924025   \n",
      "5       1000         5000   0.719338  0.334957  0.457077  3.869997   \n",
      "\n",
      "   memory_usage (MB)  \n",
      "0        2786.410156  \n",
      "1        2785.578125  \n",
      "2        2766.843750  \n",
      "3        2780.687500  \n",
      "4        2786.988281  \n",
      "5        2779.617188  \n"
     ]
    }
   ],
   "source": [
    "#Eksperyment: Algorytm Collaborative filtering oparty na pamięci\n",
    "\n",
    "# Parametry eksperymentu\n",
    "user_sizes = [50, 1000]  # Liczba użytkowników do testowania\n",
    "movie_sizes = [100, 1000, 5000]  # Liczba filmów do testowania\n",
    "\n",
    "# Wczytanie danych z MovieLens\n",
    "df = pd.read_csv(ml_rating_path)\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "# Eksperyment\n",
    "results = []\n",
    "for user_size in user_sizes:\n",
    "    # Ogranicz liczbę użytkowników\n",
    "    sampled_users = df['userId'].unique()[:user_size]\n",
    "    filtered_data = df[df['userId'].isin(sampled_users)]\n",
    "    \n",
    "    for movie_size in movie_sizes:\n",
    "        print(f\"User Size: {user_size}, Movie Count: {movie_size}\")\n",
    "        reduced_data = reduce_movies(filtered_data, movie_size)\n",
    "        \n",
    "        # Przygotowanie danych do modelu\n",
    "        data = Dataset.load_from_df(reduced_data[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
    "        \n",
    "        # Tworzenie modelu Collaborative Filtering opartego na użytkownikach\n",
    "        sim_options = {'name': 'cosine', 'user_based': True}\n",
    "        model = KNNBasic(sim_options=sim_options)\n",
    "        \n",
    "        # Pomiar czasu i zasobów\n",
    "        start_time = time()\n",
    "        model.fit(trainset)\n",
    "        predictions = model.test(testset)\n",
    "        end_time = time()\n",
    "        \n",
    "        # Obliczenie metryk\n",
    "        precision, recall, f1 = calculate_metrics(predictions)\n",
    "        \n",
    "        # Pomiar zużycia pamięci\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_usage = process.memory_info().rss / (1024 * 1024)  # Pamięć w MB\n",
    "        \n",
    "        # Zapis wyników\n",
    "        results.append({\n",
    "            'user_size': user_size,\n",
    "            'movie_count': movie_size,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'execution_time (s)': end_time - start_time,\n",
    "            'memory_usage (MB)': memory_usage\n",
    "        })\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44941146",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Size: 50, Movie Count: 100\n",
      "User Size: 50, Movie Count: 1000\n",
      "User Size: 50, Movie Count: 5000\n",
      "User Size: 1000, Movie Count: 100\n",
      "User Size: 1000, Movie Count: 1000\n",
      "User Size: 1000, Movie Count: 5000\n",
      "User Size: 5000, Movie Count: 100\n",
      "User Size: 5000, Movie Count: 1000\n",
      "User Size: 5000, Movie Count: 5000\n",
      "   user_size  movie_count  precision    recall  f1_score      time  \\\n",
      "0         50          100   0.786667  0.561905  0.655556  0.054850   \n",
      "1         50         1000   0.709302  0.398693  0.510460  0.043870   \n",
      "2         50         5000   0.789916  0.262204  0.393717  0.061828   \n",
      "3       1000          100   0.824080  0.572605  0.675703  0.107659   \n",
      "4       1000         1000   0.814847  0.395926  0.532914  0.667097   \n",
      "5       1000         5000   0.820089  0.351249  0.491841  1.516564   \n",
      "6       5000          100   0.840692  0.577434  0.684628  0.706231   \n",
      "7       5000         1000   0.838306  0.441986  0.578805  3.643217   \n",
      "8       5000         5000   0.833353  0.404854  0.544960  7.055702   \n",
      "\n",
      "   memory_usage  cpu_usage  \n",
      "0          55.3        0.0  \n",
      "1          55.2        0.0  \n",
      "2          55.1        3.6  \n",
      "3          55.1       44.8  \n",
      "4          55.3       28.6  \n",
      "5          55.3        3.6  \n",
      "6          55.4        3.6  \n",
      "7          55.6        6.9  \n",
      "8          56.6        0.0  \n"
     ]
    }
   ],
   "source": [
    "#Eksperyment: Algorytm Collaborative filtering oparty na modelu\n",
    "\n",
    "# Parametry eksperymentu\n",
    "user_sizes = [50, 1000, 5000]  # Liczba użytkowników do testowania\n",
    "movie_sizes = [100, 1000, 5000]  # Liczba filmów do testowania\n",
    "\n",
    "# Wczytanie danych z MovieLens\n",
    "df = pd.read_csv(ml_rating_path)\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "\n",
    "# Eksperyment\n",
    "results = []\n",
    "for user_size in user_sizes:\n",
    "    # Ogranicz liczbę użytkowników\n",
    "    sampled_users = df['userId'].unique()[:user_size]\n",
    "    filtered_data = df[df['userId'].isin(sampled_users)]\n",
    "    \n",
    "    for movie_size in movie_sizes:\n",
    "        print(f\"User Size: {user_size}, Movie Count: {movie_size}\")\n",
    "        reduced_data = reduce_movies(filtered_data, movie_size)\n",
    "        \n",
    "        # Przygotowanie danych do modelu\n",
    "        data = Dataset.load_from_df(reduced_data[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
    "        \n",
    "        # Tworzenie modelu Collaborative Filtering opartego na SVD\n",
    "        model = SVD()\n",
    "        \n",
    "        # Pomiar czasu i zasobów\n",
    "        start_time = time()\n",
    "        model.fit(trainset)\n",
    "        predictions = model.test(testset)\n",
    "        end_time = time()\n",
    "        \n",
    "        # Obliczenie metryk\n",
    "        precision, recall, f1 = calculate_metrics(predictions)\n",
    "        \n",
    "        # Pomiar zasobów (pamięci w MB)\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_usage = process.memory_info().rss / (1024 * 1024)  # Pamięć w MB\n",
    "        \n",
    "        # Zapis wyników\n",
    "        results.append({\n",
    "            'user_size': user_size,\n",
    "            'movie_count': movie_size,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'execution_time (s)': end_time - start_time,\n",
    "            'memory_usage (MB)': memory_usage  # Zużycie pamięci w MB\n",
    "        })\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb2afff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Size: 10, Movie Size: 50\n",
      "User Size: 10, Movie Size: 1000\n",
      "User Size: 10, Movie Size: 10000\n",
      "   user_size  movie_size  precision    recall  f1_score       time  \\\n",
      "0         10          50      0.000  0.000000  0.000000   0.005981   \n",
      "1         10        1000      0.000  0.000000  0.000000   0.110673   \n",
      "2         10       10000      0.002  0.013514  0.003484  15.964484   \n",
      "\n",
      "   memory_usage  cpu_usage  \n",
      "0          52.8        4.2  \n",
      "1          52.8       35.7  \n",
      "2          52.8        0.0  \n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e042a2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with user_sample_size=100 and movie_sample_size=100\n",
      "Running experiment with user_sample_size=100 and movie_sample_size=5000\n",
      "Running experiment with user_sample_size=100 and movie_sample_size=20000\n",
      "Running experiment with user_sample_size=1000 and movie_sample_size=100\n",
      "Running experiment with user_sample_size=1000 and movie_sample_size=5000\n",
      "Running experiment with user_sample_size=1000 and movie_sample_size=20000\n",
      "   user_sample_size  movie_sample_size  precision  recall  f1_score  \\\n",
      "0               100                100     0.1980     1.0  0.330551   \n",
      "1               100               5000     0.4270     1.0  0.598458   \n",
      "2               100              20000     0.4490     1.0  0.619738   \n",
      "3              1000                100     0.1936     1.0  0.324397   \n",
      "4              1000               5000     0.4172     1.0  0.588767   \n",
      "5              1000              20000     0.4765     1.0  0.645445   \n",
      "\n",
      "   memory_usage (MB)  execution_time (s)  \n",
      "0        3521.687500            8.512706  \n",
      "1        3666.109375            9.933101  \n",
      "2        5737.046875           14.348552  \n",
      "3        3512.640625           88.077223  \n",
      "4        3660.683594          106.570622  \n",
      "5        5734.851562          115.003692  \n",
      "End\n"
     ]
    }
   ],
   "source": [
    "#Eksperyment: Algorytm Content-based filtering\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "ml_rating_path = 'dane/ml-20m/ml-20m/ratings.csv'\n",
    "ml_movies_path = 'dane/ml-20m/ml-20m/movies.csv'\n",
    "ml_tags_path = 'dane/ml-20m/ml-20m/tags.csv'\n",
    "\n",
    "# Wczytanie danych\n",
    "ratings = pd.read_csv(ml_rating_path)\n",
    "movies = pd.read_csv(ml_movies_path)\n",
    "tags = pd.read_csv(ml_tags_path)\n",
    "\n",
    "# Przygotowanie cech filmów: gatunki + tagi\n",
    "movies['genres'] = movies['genres'].fillna('')\n",
    "tags['tag'] = tags['tag'].fillna('')\n",
    "\n",
    "# Grupowanie tagów według movieId i łączenie ich w jeden ciąg\n",
    "tags_combined = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Łączenie danych\n",
    "movies = movies.merge(tags_combined, on='movieId', how='left')\n",
    "\n",
    "# Tworzenie cechy 'features' (gatunki + tagi)\n",
    "movies['features'] = movies['genres'] + ' ' + movies['tag']\n",
    "movies['features'] = movies['features'].fillna('')  # Uzupełnianie brakujących wartości\n",
    "\n",
    "# Filtrowanie ocen, aby pasowały do istniejących filmów\n",
    "ratings = ratings[ratings['movieId'].isin(movies['movieId'])]\n",
    "\n",
    "# Podział danych na zbiór treningowy i testowy\n",
    "ratings_train, ratings_test = train_test_split(ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# Funkcja do tworzenia profilu użytkownika (na danych treningowych)\n",
    "def create_user_profile(user_id, ratings, cosine_sim, movies_sample, tfidf_matrix):\n",
    "    rated_movies = ratings[ratings['userId'] == user_id]\n",
    "    weighted_features = np.zeros(tfidf_matrix.shape[1])  # Wektor cech filmów\n",
    "    total_weight = 0\n",
    "    \n",
    "    for movie_id, rating in zip(rated_movies['movieId'], rated_movies['rating']):\n",
    "        # Znajdowanie indeksu w nowej próbce\n",
    "        movie_idx = movies_sample[movies_sample['movieId'] == movie_id].index\n",
    "        if not movie_idx.empty:  # Film istnieje w ograniczonej próbce\n",
    "            idx = movie_idx[0]  # Indeks w macierzy TF-IDF\n",
    "            weighted_features += tfidf_matrix[idx].toarray().flatten() * rating\n",
    "            total_weight += rating\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        weighted_features /= total_weight  # Normalizacja\n",
    "    \n",
    "    return weighted_features\n",
    "\n",
    "# Funkcja do generowania rekomendacji na podstawie profilu użytkownika\n",
    "def get_recommendations(user_id, ratings, cosine_sim, movies_sample, tfidf_matrix, top_n=10):\n",
    "    user_profile = create_user_profile(user_id, ratings, cosine_sim, movies_sample, tfidf_matrix)\n",
    "    \n",
    "    # Obliczamy podobieństwo profilu użytkownika do każdego filmu\n",
    "    similarities = cosine_similarity([user_profile], tfidf_matrix)[0]\n",
    "    \n",
    "    # Sortujemy filmy po podobieństwie\n",
    "    similar_movie_indices = similarities.argsort()[-top_n-1:-1][::-1]\n",
    "    recommended_movie_ids = movies_sample.iloc[similar_movie_indices]['movieId'].tolist()\n",
    "    \n",
    "    return recommended_movie_ids\n",
    "\n",
    "# Funkcja do obliczania metryk\n",
    "def calculate_metrics(recommended_movies, ratings_test):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for movie in recommended_movies:\n",
    "        actual_ratings = ratings_test[ratings_test['movieId'] == movie]\n",
    "        if not actual_ratings.empty:\n",
    "            user_rating = actual_ratings['rating'].iloc[0]\n",
    "            y_true.append(1 if user_rating >= 4 else 0)\n",
    "        else:\n",
    "            y_true.append(0)\n",
    "        y_pred.append(1)  # Zawsze zakładamy, że rekomendacja jest pozytywna\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Funkcja do przeprowadzenia eksperymentu\n",
    "def run_experiment(ratings_train, ratings_test, movies, movie_sample_size, user_sample_size=10, top_n=10):\n",
    "    # Losowe próbkowanie filmów\n",
    "    movies_sample = movies.sample(n=movie_sample_size, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Macierz podobieństw dla próbki filmów\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = vectorizer.fit_transform(movies_sample['features'])\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "    \n",
    "    # Tworzymy próbkę użytkowników\n",
    "    users_sample = ratings_train['userId'].drop_duplicates().sample(user_sample_size, random_state=42)\n",
    "    \n",
    "    # Generowanie rekomendacji dla użytkowników\n",
    "    all_recommendations = []\n",
    "    all_actual_movies = []\n",
    "    \n",
    "    for user in users_sample:\n",
    "        recommended_movies = get_recommendations(user, ratings_train, cosine_sim, movies_sample, tfidf_matrix, top_n=top_n)\n",
    "        actual_movies = ratings_test[ratings_test['userId'] == user]['movieId'].tolist()\n",
    "        \n",
    "        all_recommendations.extend(recommended_movies)\n",
    "        all_actual_movies.extend(actual_movies)\n",
    "\n",
    "    # Obliczanie metryk\n",
    "    precision, recall, f1 = calculate_metrics(all_recommendations, ratings_test)\n",
    "\n",
    "    # Zużycie pamięci\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_usage = process.memory_info().rss / (1024 * 1024)\n",
    "\n",
    "    return precision, recall, f1, memory_usage\n",
    "\n",
    "# Przeprowadzanie eksperymentu\n",
    "user_sample_sizes = [100, 1000]  # Liczba użytkowników\n",
    "movie_sample_sizes = [100, 5000, 20000]  # Liczba filmów\n",
    "\n",
    "# Zapisanie wyników eksperymentu\n",
    "results = []\n",
    "\n",
    "for user_sample_size in user_sample_sizes:\n",
    "    for movie_sample_size in movie_sample_sizes:\n",
    "        print(f\"Running experiment with user_sample_size={user_sample_size} and movie_sample_size={movie_sample_size}\")\n",
    "        \n",
    "        # Mierzymy czas przed rozpoczęciem eksperymentu\n",
    "        start_time = time()\n",
    "\n",
    "        # Uruchamiamy eksperyment\n",
    "        precision, recall, f1, memory_usage = run_experiment(\n",
    "            ratings_train=ratings_train, \n",
    "            ratings_test=ratings_test, \n",
    "            movies=movies, \n",
    "            movie_sample_size=movie_sample_size, \n",
    "            user_sample_size=user_sample_size\n",
    "        )\n",
    "\n",
    "        # Mierzymy czas po zakończeniu eksperymentu\n",
    "        end_time = time()\n",
    "        \n",
    "        #Zapis wyników\n",
    "        results.append({\n",
    "            'user_sample_size': user_sample_size,\n",
    "            'movie_sample_size': movie_sample_size,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'memory_usage (MB)': memory_usage,\n",
    "            'execution_time (s)': end_time - start_time\n",
    "        })\n",
    "\n",
    "# Wyświetlanie wyników\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8efb837f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Size: 50, Movie Count: 100\n",
      "User Size: 50, Movie Count: 1000\n",
      "User Size: 50, Movie Count: 10000\n",
      "User Size: 1000, Movie Count: 100\n",
      "User Size: 1000, Movie Count: 1000\n",
      "User Size: 1000, Movie Count: 10000\n",
      "   user_size  movie_count  precision  recall  f1_score  memory_usage (MB)  \\\n",
      "0         50          100     0.0660     1.0  0.123827        1552.160156   \n",
      "1         50         1000     0.4680     1.0  0.637602        1554.699219   \n",
      "2         50        10000     0.8040     1.0  0.891353        1572.222656   \n",
      "3       1000          100     0.0152     1.0  0.029945        1282.441406   \n",
      "4       1000         1000     0.4584     1.0  0.628634        1290.082031   \n",
      "5       1000        10000     0.7108     1.0  0.830956        1307.277344   \n",
      "\n",
      "   execution_time (s)  \n",
      "0            1.950281  \n",
      "1            2.908201  \n",
      "2            5.517966  \n",
      "3           46.560652  \n",
      "4           61.572669  \n",
      "5          131.547391  \n"
     ]
    }
   ],
   "source": [
    "#Eksperyment: Algorytm Hybrid Filtering łączący Content-based filtering i collabroative filtering oparty na modelu. Ważona\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise import accuracy\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from time import time\n",
    "import psutil\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "\n",
    "# Wczytanie danych\n",
    "ml_rating_path = 'dane/ml-20m/ml-20m/ratings.csv'\n",
    "ml_movies_path = 'dane/ml-20m/ml-20m/movies.csv'\n",
    "ml_tags_path = 'dane/ml-20m/ml-20m/tags.csv'\n",
    "\n",
    "ratings = pd.read_csv(ml_rating_path)\n",
    "movies = pd.read_csv(ml_movies_path)\n",
    "tags = pd.read_csv(ml_tags_path)\n",
    "\n",
    "# Przygotowanie cech filmów: gatunki + tagi\n",
    "movies['genres'] = movies['genres'].fillna('')\n",
    "tags['tag'] = tags['tag'].fillna('')\n",
    "\n",
    "# Grupowanie tagów według movieId i łączenie ich w jeden ciąg\n",
    "tags_combined = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Łączenie danych\n",
    "movies = movies.merge(tags_combined, on='movieId', how='left')\n",
    "\n",
    "# Tworzenie cechy 'features' (gatunki + tagi)\n",
    "movies['features'] = movies['genres'] + ' ' + movies['tag']\n",
    "movies['features'] = movies['features'].fillna('')\n",
    "\n",
    "# Funkcja do ograniczenia liczby filmów\n",
    "def reduce_movies(data, movie_count):\n",
    "    sampled_movies = data['movieId'].unique()[:movie_count]\n",
    "    reduced_data = data[data['movieId'].isin(sampled_movies)]\n",
    "    return reduced_data\n",
    "\n",
    "# Funkcja do tworzenia profilu użytkownika (na danych treningowych)\n",
    "def create_user_profile(user_id, ratings, cosine_sim, movies_sample, tfidf_matrix):\n",
    "    rated_movies = ratings[ratings['userId'] == user_id]\n",
    "    weighted_features = np.zeros(tfidf_matrix.shape[1])  # Wektor cech filmów\n",
    "    total_weight = 0\n",
    "    \n",
    "    for movie_id, rating in zip(rated_movies['movieId'], rated_movies['rating']):\n",
    "        # Znajdowanie indeksu w nowej próbce\n",
    "        movie_idx = movies_sample[movies_sample['movieId'] == movie_id].index\n",
    "        if not movie_idx.empty:  # Film istnieje w ograniczonej próbce\n",
    "            idx = movie_idx[0]  # Indeks w macierzy TF-IDF\n",
    "            weighted_features += tfidf_matrix[idx].toarray().flatten() * rating\n",
    "            total_weight += rating\n",
    "    \n",
    "    if total_weight > 0:\n",
    "        weighted_features /= total_weight  # Normalizacja\n",
    "    \n",
    "    return weighted_features\n",
    "\n",
    "# Funkcja do generowania rekomendacji z wagami\n",
    "def get_hybrid_recommendations(user_id, ratings, cosine_sim, movies_sample, tfidf_matrix, svd_model, w_content=0.8, w_collab=0.2, top_n=10):\n",
    "    # Generowanie rekomendacji content-based\n",
    "    user_profile = create_user_profile(user_id, ratings, cosine_sim, movies_sample, tfidf_matrix)\n",
    "    similarities = cosine_similarity([user_profile], tfidf_matrix)[0]\n",
    "    content_based_recs = similarities.argsort()[-(top_n*10):][::-1]\n",
    "    content_based_movie_ids = movies_sample.iloc[content_based_recs]['movieId'].tolist()\n",
    "\n",
    "    # Generowanie rekomendacji collaborative filtering\n",
    "    svd_recs = [svd_model.predict(user_id, movie_id).est for movie_id in movies_sample['movieId']]\n",
    "    svd_movie_ids = np.argsort(svd_recs)[-(top_n*10):][::-1]\n",
    "    svd_movie_ids = movies_sample.iloc[svd_movie_ids]['movieId'].tolist()\n",
    "\n",
    "    # Połączenie wyników na podstawie wag\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    # Łączenie wyników obu metod (Content-based + Collaborative Filtering)\n",
    "    for movie_id in set(content_based_movie_ids + svd_movie_ids):\n",
    "        # Content-based score\n",
    "        content_score = w_content * (similarities[movies_sample['movieId'] == movie_id][0] if movie_id in content_based_movie_ids else 0)\n",
    "        \n",
    "        # Collaborative filtering score\n",
    "        collab_score = 0\n",
    "        if movie_id in svd_movie_ids:\n",
    "            idx = movies_sample[movies_sample['movieId'] == movie_id].index[0]\n",
    "            collab_score = w_collab * svd_recs[idx] / 5\n",
    "\n",
    "        # Obliczenie łącznego wyniku\n",
    "        hybrid_scores[movie_id] = content_score + collab_score\n",
    "    \n",
    "    # Sortowanie rekomendacji według łącznego wyniku\n",
    "    sorted_hybrid_recs = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Wybór top_n rekomendacji\n",
    "    hybrid_recs = [movie_id for movie_id, _ in sorted_hybrid_recs[:top_n]]\n",
    "    \n",
    "    return hybrid_recs\n",
    "\n",
    "# Funkcja do obliczania metryk\n",
    "def calculate_metrics(recommended_movies, ratings_test):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for movie in recommended_movies:\n",
    "        actual_ratings = ratings_test[ratings_test['movieId'] == movie]\n",
    "        if not actual_ratings.empty:\n",
    "            user_rating = actual_ratings['rating'].iloc[0]\n",
    "            y_true.append(1 if user_rating >= 4 else 0)\n",
    "        else:\n",
    "            y_true.append(0)\n",
    "        y_pred.append(1)  # Zawsze zakładamy, że rekomendacja jest pozytywna\n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=1)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Eksperyment\n",
    "# Przygotowanie danych do eksperymentu\n",
    "user_sizes = [50, 1000]  # Liczba użytkowników do testowania\n",
    "movie_sizes = [100, 1000, 10000]  # Liczba filmów do testowania\n",
    "\n",
    "results = []\n",
    "\n",
    "for user_size in user_sizes:\n",
    "    sampled_users = ratings['userId'].unique()[:user_size]\n",
    "    filtered_data = ratings[ratings['userId'].isin(sampled_users)]\n",
    "    \n",
    "    for movie_size in movie_sizes:\n",
    "        print(f\"User Size: {user_size}, Movie Count: {movie_size}\")\n",
    "        reduced_data = reduce_movies(filtered_data, movie_size)\n",
    "        \n",
    "        # Przygotowanie danych do modelu collaborative filtering\n",
    "        reader = Reader(rating_scale=(0.5, 5.0))\n",
    "        data = Dataset.load_from_df(reduced_data[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.25, random_state=42)\n",
    "        svd_model = SVD()\n",
    "        svd_model.fit(trainset)\n",
    "        \n",
    "        # Przygotowanie danych do content-based filtering\n",
    "        movies_sample = movies.sample(n=movie_size, random_state=42).reset_index(drop=True)\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(movies_sample['features'])\n",
    "        \n",
    "        # Mierzenie czasu i zasobów\n",
    "        start_time = time()\n",
    "        \n",
    "        # Generowanie rekomendacji dla użytkowników\n",
    "        all_recommendations = []\n",
    "        all_actual_movies = []\n",
    "        \n",
    "        for user_id in sampled_users[:user_size]:\n",
    "            recommended_movies = get_hybrid_recommendations(user_id, reduced_data, None, movies_sample, tfidf_matrix, svd_model, w_content=0.7, w_collab=0.3, top_n=10)\n",
    "            actual_movies = filtered_data[filtered_data['userId'] == user_id]['movieId'].tolist()\n",
    "            \n",
    "            all_recommendations.extend(recommended_movies)\n",
    "            all_actual_movies.extend(actual_movies)\n",
    "\n",
    "        # Obliczanie metryk\n",
    "        precision, recall, f1 = calculate_metrics(all_recommendations, filtered_data)\n",
    "        \n",
    "        # Zużycie pamięci\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_usage = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "        \n",
    "        # Zapis wyników\n",
    "        end_time = time()\n",
    "        results.append({\n",
    "            'user_size': user_size,\n",
    "            'movie_count': movie_size,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'memory_usage (MB)': memory_usage,\n",
    "            'execution_time (s)': end_time - start_time\n",
    "        })\n",
    "\n",
    "# Wyświetlenie wyników\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee7f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
